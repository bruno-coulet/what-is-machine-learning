# Index

- [Mean Square Residuals](#mean-square-residuals)
- [MSE (Mean Squared Error)](#mse)
- [DÃ©rivÃ©e partielle par rapport Ã  un paramÃ¨tre](#derivÃ©e-partielle-par-rapport-Ã -un-paramÃ¨tre-theta_j)
- [DÃ©rivÃ©e partielle](#derivÃ©e-partielle)
- [Vecteur Gradient du MSE](#vecteur-gradient-du-mse)
- [Coefficient de Pearson](#coefficient-de-pearson)

---

### Mean Square Residuals
$$
MSR(\theta) = \frac{1}{m} \sum_{i=1}^{m} (\theta^T x^{(i)} - y^{(i)})^2
$$

$$
\begin{array}{|c|c|}
\hline
\textbf{Symbole} & \textbf{Signification} \\
\hline
m & \text{Nombre total d'exemples dans l'ensemble d'entraÃ®nement} \\
x^{(i)} & \text{Vecteur des caractÃ©ristiques de l'exemple } i \\
\theta & \text{Vecteur des paramÃ¨tres du modÃ¨le} \\
\theta^T x^{(i)} \text{ ou } h (x^{(i)}) & \text{PrÃ©diction du modÃ¨le pour } x^{(i)} \\
y^{(i)} & \text{Valeur rÃ©elle associÃ©e Ã  } x^{(i)} \\
(\theta^T x^{(i)} - y^{(i)})^2 & \text{Erreur quadratique pour un exemple donnÃ©} \\
\hline
\end{array}
$$

### MSE
**Fonction de coÃ»t  pour le modÃ¨le de rÃ©gression linÃ©aire**

Voir la [fonction de coÃ»t](#regression-lineaire.md) du modÃ¨le de regression linÃ©aire

Peut s'Ã©crire de plusieurs maniÃ¨res :<br>
$MSE(X, h_\theta)$, pour montrer que le modÃ¨le est paramÃ©trÃ© par le vecteur $\theta$<br>$MSE(X, h)$<br>
$MSE(\theta)$ pour simplifier
$$
MSE(X, h_\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( \theta^T x^{(i)} - y^{(i)} \right)^2
$$

S'Ã©crit aussi

$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2$$


|symbole|signification|
|:--:|:--------|
|$m$ | nombre total d'exemples dans l'ensemble d'entraÃ®nement.|
|$ğ‘¥(ğ‘–)$  | vecteur des caractÃ©ristiques de l'exemple ğ‘–|
|$ğœƒ$ | vecteur des paramÃ¨tres du modÃ¨le.|
|$ğœƒğ‘‡ğ‘¥(ğ‘–)$ <font color = "orange">ou</font> $h (x^{(i)})$ | prÃ©diction du modÃ¨le pour $ğ‘¥(ğ‘–)$|
|$y(i)$|valeur rÃ©elle associÃ©e Ã  $ğ‘¥_i$|
|$ (\theta^T x^{(i)} - y^{(i)})^2 $|erreur quadratique pour un exemple donnÃ©|

---
## DÃ©rivÃ©e partielle par rapport Ã  un paramÃ¨tre $\theta_j$

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)}$$


## DÃ©rivÃ©e partielle
On note $â„ğœƒ(x^{(  i)})=ğœƒ^ğ‘‡ğ‘¥^{(ğ‘–)}$<br><br>
Donc la dÃ©rivÃ©e partielle du MSE par rapport Ã  $\theta_j$  est :


$$
\frac{\partial MSE}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m}2 \left( \theta^T x^{(i)} - y^{(i)} \right) x_j^{(i)}
$$

DÃ©rivÃ©e partielle par rapport Ã  $\theta_j$ simplifiÃ©e

$$
\frac{\partial MSE}{\partial \theta_j} = \frac{2}{m} \sum_{i=1}^{m} \left( \theta^T x^{(i)} - y^{(i)} \right) x_j^{(i)}
$$




| Symbole               | Signification |
|-----------------------|--------------|
| $MSE(X, h_\theta) $  | Erreur quadratique moyenne (Mean Squared Error) |
| $\theta $        | Vecteur des paramÃ¨tres du modÃ¨le |
| $\theta_j $      | $j $-iÃ¨me paramÃ¨tre du modÃ¨le |
| $\theta^T x^{(i)} $ | Produit scalaire entre $\theta $ et $x^{(i)} $, soit la prÃ©diction du modÃ¨le |
| $m $            | Nombre total d'exemples d'entraÃ®nement |
| $x^{(i)} $      | Vecteur des caractÃ©ristiques de l'exemple $i $ |
| $x_j^{(i)} $    | $j $-iÃ¨me caractÃ©ristique de l'exemple $i $ |
| $y^{(i)} $      | Valeur rÃ©elle de sortie pour l'exemple $i $ |
| $h_\theta(x^{(i)}) $ | PrÃ©diction du modÃ¨le pour l'exemple $i $ (Ã©quivalent Ã  $\theta^T x^{(i)} $) |
| $\alpha $      | Taux d'apprentissage (learning rate) |


---
### Vecteur Gradient du MSE

Pour calculer
**Vecteur Gradient du MSE**
$$
\nabla_\theta MSE = \frac{2}{m} X^T (X\theta - y)
$$




| Symbole                    | Signification |
|----------------------------|--------------|
| $ \nabla_\theta MSE $      | Gradient du MSE (vecteur des dÃ©rivÃ©es partielles) |
| $ X $                      | Matrice des caractÃ©ristiques de taille $ m \times n $ |
| $ y $                      | Vecteur des valeurs rÃ©elles de taille $ m \times 1 $ |
| $ \theta $                 | Vecteur des paramÃ¨tres du modÃ¨le de taille $ n \times 1 $ |
| $ X\theta $                | PrÃ©dictions du modÃ¨le (produit matriciel) de taille $ m \times 1 $ |
| $ X^T $                    | TransposÃ©e de la matrice $ X $, de taille $ n \times m $ |
| $ X^T (X\theta - y) $      | Gradient du MSE avant multiplication par $ \frac{2}{m} $ |
| $ \alpha $                 | Taux d'apprentissage (learning rate) |

---

### coefficient de Pearson
â€‹$$ r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}} $$


- **r** : le coefficient de corrÃ©lation linÃ©aire de Pearson  
- **$ x_i $** et **$ y_i $** : les valeurs des deux variables Ã©tudiÃ©es  
- **$ \bar{x} $** et **$ \bar{y} $** : les moyennes des variables $ x $et $ y $ 
- Le numÃ©rateur mesure la covariance entre $ x $et $ y $ 
- Le dÃ©nominateur normalise cette covariance par le produit des Ã©carts-types des deux variables  
<br>
[Retour Ã  l'index](#index)