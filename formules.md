# Index

- [Mean Square Residuals](#mean-square-residuals)
- [MSE (Mean Squared Error)](#mse)
- [D√©riv√©e partielle par rapport √† un param√®tre](#deriv√©e-partielle-par-rapport-√†-un-param√®tre-theta_j)
- [D√©riv√©e partielle](#deriv√©e-partielle)
- [Vecteur Gradient du MSE](#vecteur-gradient-du-mse)
- [Coefficient de Pearson](#coefficient-de-pearson)

---
$$
\begin{array}{|c|l|}
\hline
\textbf{Symbole} & \textbf{Signification} \\
\hline
m  & \text{Nombre d'observation du jeu de donn√©e} \\
\hline
\boldsymbol{x}^i & \text{Vecteur  constitu√© des valeurs de des variables (hors √©tiquette) pour la i√®me observation} \\
\hline
\boldsymbol{X}  & \text{Matrice contenant toutes les valeurs des toutes les variable (hors √©tiquette) du jeu de donn√©e} \\
\hline
h & \text{Hypoth√®se, fonction de pr√©diction} \\
\hline
\hat{y} & \text{y chapeau, valeur pr√©dite, retourn√©e par }h(x^i)\\
\hline
T & \text{Transpos√©e}\\
\hline
\end{array}
$$

En alg√®bre lin√©aire, par convention, un vecteur est g√©n√©ralement consid√©r√© comme un vecteur colonne.
$$
\begin{array}{|c|c|}
\hline
\textbf{Vecteur } \boldsymbol{x}^i & \textbf{Transpos√©e } (\boldsymbol{x}^i)^T \\
\hline
\boldsymbol{x}^i =\begin{pmatrix} -11 \\ 45 \\ 62 \\ 13 \end{pmatrix} & 
(\boldsymbol{x}^i)^T =\begin{pmatrix} -11 & 45 & 62 & 13 \end{pmatrix} \\
\hline
\end{array}
$$

Matrice $\boldsymbol{X}$, constitu√©e de m vecteurs colonnes, chacun transpos√© en vecteur ligne
$$
\boldsymbol{X} = 
\begin{pmatrix} (\boldsymbol{x}^1)^T \\ (\boldsymbol{x}^2)^T \\ (\boldsymbol{x}^{...})^T \\ (\boldsymbol{x}^{m})^T \end{pmatrix} =  
\begin{pmatrix} -11 & 45 & 62 & 13 \\ \cdot & \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot & \cdot \\ \cdot & \cdot & \cdot & \cdot  \end{pmatrix}
$$










### Mean Square Residuals - R√©sidu Quadratique Moyen
$$
MSR(\theta) = \frac{1}{m} \sum_{i=1}^{m} (\theta^T x^{(i)} - y^{(i)})^2
$$

$$
\begin{array}{|c|c|}
\hline
\textbf{Symbole} & \textbf{Signification} \\
\hline
m & \text{Nombre total d'exemples dans l'ensemble d'entra√Ænement} \\
x^{(i)} & \text{Vecteur des caract√©ristiques de l'exemple } i \\
\theta & \text{Vecteur des param√®tres du mod√®le} \\
\theta^T x^{(i)} \text{ ou } h (x^{(i)}) & \text{Pr√©diction du mod√®le pour } x^{(i)} \\
y^{(i)} & \text{Valeur r√©elle associ√©e √† } x^{(i)} \\
(\theta^T x^{(i)} - y^{(i)})^2 & \text{Erreur quadratique pour un exemple donn√©} \\
\hline
\end{array}
$$

### Mean Square Error - Erreure Quadratique moyenne
**Fonction de co√ªt  pour le mod√®le de r√©gression lin√©aire**

Voir la [fonction de co√ªt](#regression-lineaire.md) du mod√®le de regression lin√©aire

Peut s'√©crire de plusieurs mani√®res :<br>
$MSE(X, h_\theta)$, pour montrer que le mod√®le est param√©tr√© par le vecteur $\theta$<br>$MSE(X, h)$<br>
$MSE(\theta)$ pour simplifier
$$
MSE(X, h_\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( \theta^T x^{(i)} - y^{(i)} \right)^2
$$

S'√©crit aussi

$$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right)^2$$


|symbole|signification|
|:--:|:--------|
|$m$ | nombre total d'exemples dans l'ensemble d'entra√Ænement.|
|$ùë•(ùëñ)$  | vecteur des caract√©ristiques de l'exemple ùëñ|
|$ùúÉ$ | vecteur des param√®tres du mod√®le.|
|$ùúÉùëáùë•(ùëñ)$ <font color = "orange">ou</font> $h (x^{(i)})$ | pr√©diction du mod√®le pour $ùë•(ùëñ)$|
|$y(i)$|valeur r√©elle associ√©e √† $ùë•_i$|
|$(\theta^T x^{(i)} - y^{(i)})^2$|erreur quadratique pour un exemple donn√©|

---
## D√©riv√©e partielle par rapport √† un param√®tre $\theta_j$

$$\frac{\partial J(\theta)}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m} \left( h_\theta(x^{(i)}) - y^{(i)} \right) x_j^{(i)}$$


## D√©riv√©e partielle
On note $‚ÑéùúÉ(x^{(  i)})=ùúÉ^ùëáùë•^{(ùëñ)}$<br><br>
Donc la d√©riv√©e partielle du MSE par rapport √† $\theta_j$  est :


$$
\frac{\partial MSE}{\partial \theta_j} = \frac{1}{m} \sum_{i=1}^{m}2 \left( \theta^T x^{(i)} - y^{(i)} \right) x_j^{(i)}
$$

D√©riv√©e partielle par rapport √† $\theta_j$ simplifi√©e

$$
\frac{\partial MSE}{\partial \theta_j} = \frac{2}{m} \sum_{i=1}^{m} \left( \theta^T x^{(i)} - y^{(i)} \right) x_j^{(i)}
$$




| Symbole               | Signification |
|-----------------------|--------------|
| $MSE(X, h_\theta) $  | Erreur quadratique moyenne (Mean Squared Error) |
| $\theta $        | Vecteur des param√®tres du mod√®le |
| $\theta_j $      | $j $-i√®me param√®tre du mod√®le |
| $\theta^T x^{(i)} $ | Produit scalaire entre $\theta $ et $x^{(i)} $, soit la pr√©diction du mod√®le |
| $m $            | Nombre total d'exemples d'entra√Ænement |
| $x^{(i)} $      | Vecteur des caract√©ristiques de l'exemple $i $ |
| $x_j^{(i)} $    | $j $-i√®me caract√©ristique de l'exemple $i $ |
| $y^{(i)} $      | Valeur r√©elle de sortie pour l'exemple $i $ |
| $h_\theta(x^{(i)}) $ | Pr√©diction du mod√®le pour l'exemple $i $ (√©quivalent √† $\theta^T x^{(i)} $) |
| $\alpha $      | Taux d'apprentissage (learning rate) |


---
### Vecteur Gradient du MSE

Pour calculer
**Vecteur Gradient du MSE**
$$
\nabla_\theta MSE = \frac{2}{m} X^T (X\theta - y)
$$




| Symbole                    | Signification |
|----------------------------|--------------|
| $\nabla_\theta MSE$      | Gradient du MSE (vecteur des d√©riv√©es partielles) |
| $X$                      | Matrice des caract√©ristiques de taille $ m \times n $ |
| $y$                      | Vecteur des valeurs r√©elles de taille $ m \times 1 $ |
| $\theta$                 | Vecteur des param√®tres du mod√®le de taille $ n \times 1 $ |
| $X\theta$                | Pr√©dictions du mod√®le (produit matriciel) de taille $ m \times 1 $ |
| $X^T$                    | Transpos√©e de la matrice $ X $, de taille $ n \times m $ |
| $X^T (X\theta - y)$      | Gradient du MSE avant multiplication par $ \frac{2}{m} $ |
| $\alpha$                 | Taux d'apprentissage (learning rate) |

---

### coefficient de Pearson
‚Äã$$r = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2 \sum (y_i - \bar{y})^2}}$$


- **r** : le coefficient de corr√©lation lin√©aire de Pearson  
- **$x_i$** et **$y_i$** : les valeurs des deux variables √©tudi√©es  
- **$ \bar{x} $** et **$ \bar{y} $** : les moyennes des variables $x$ et $y$ 
- Le num√©rateur mesure la covariance entre $x$et $y$ 
- Le d√©nominateur normalise cette covariance par le produit des √©carts-types des deux variables  
<br>
[Retour √† l'index](#index)
