[Retour au README / regression](README.md#regression-lineaire)

# RÃ©gression linÃ©aire
mÃ©thode simple qui modÃ©lise la relation entre une variable dÃ©pendante (en sortie) et une ou plusieurs variables indÃ©pendantes (en entrÃ©e) par une droite.

En apprentissage supervisÃ©, la rÃ©gression linÃ©aire vise Ã  approximer une relation linÃ©aire entre les variables explicatives 
$ğ‘‹$ et la variable cible $ğ‘¦$.

**prÃ©diction =  somme pondÃ©rÃ©e des variables d'entrÃ©e plus une constante**

Peut s'Ã©crire sous plusieurs formes:
- [scalaire](#Forme-scalaire)
- [pondÃ©rÃ©e](#Forme-somme-pondÃ©rÃ©e)
- [vectorielle](#Forme-vectorielle)
- [matricielle](#Forme-matricielle)

## Regression LinÃ©aire - Entrainement

Entrainer un modÃ¨le consiste Ã  dÃ©finir ses paramÃ¨tres de telle sorte qu'ils s'ajustent au mieux au jeu d'entrainement

Mesures courantes pour indiquer si un modÃ¨le de Regression s'ajuste bien ou pas aux donnÃ©e d'entrainement :

- RMSE racine carrÃ©e des erreurs quadratique moyenne -(root mean square error)
- MSE erreur quadratique moyenne - (mean square error)


Pour entrainer un modÃ¨le de rÃ©gression linÃ©aire, il faut donc trouver le vecteur Î¸ qui minimise la RMSE

En pratique, il est plus simple de minimiser la MSE que la RMSE

La qualitÃ© de cette approximation est mesurÃ©e Ã  l'aide de la fonction de coÃ»t, qui quantifie l'erreur entre les prÃ©dictions du modÃ¨le et les vraies valeurs.

### Fonction de coÃ»t MSE pour le modÃ¨le de rÃ©gression linÃ©aire


$$
MSE(X, h_\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( \theta^T x^{(i)} - y^{(i)} \right)^2
$$

$(X, h_\theta)$ pour montrer que le modÃ¨le est paramÃ©trÃ© par le vecteur $\theta$<br><br>
Peut s'Ã©crire de plusieurs maniÃ¨res :<br>
$MSE(X, h_\theta)$<br>$MSE(X, h)$ comme pour la RMSE ci-dessous<br>$MSE(\theta)$ pour simplifier

|symbole|signification|
|:--:|:--------|
|$m$ | nombre total d'exemples dans l'ensemble d'entraÃ®nement.|
|$ğ‘¥(ğ‘–)$  | vecteur des caractÃ©ristiques de l'exemple ğ‘–|
|$ğœƒ$ | vecteur des paramÃ¨tres du modÃ¨le.|
|$ğœƒğ‘‡ğ‘¥(ğ‘–)$ <font color = "orange">ou</font> $h (x^{(i)})$ | prÃ©diction du modÃ¨le pour $ğ‘¥(ğ‘–)$|
|$y(i)$|valeur rÃ©elle associÃ©e Ã  $ğ‘¥_i$|
|$ (\theta^T x^{(i)} - y^{(i)})^2 $|erreur quadratique pour un exemple donnÃ©|

<font color = "orange">Le MSE pÃ©nalise les grandes erreurs plus fortement que les petites erreurs, ce qui le rend sensible aux valeurs aberrantes (outliers).</font>

### Fonction de coÃ»t MSE pour le modÃ¨le de rÃ©gression linÃ©aire

$$ RMSE(X, h) = \sqrt{\frac{1}{m} \sum_{i=1}^{m} \left( h (x^{(i)}) - y^{(i)} \right)^2} $$



### Equation normale - solution analytique
formule mathÃ©matique qui donne lavaleur minimal de $\theta$
$$ \hat{\theta} = (X^T X)^{-1} X^T y $$

| Symbole                            | signification |
|----------------------------------------|---------|
| $ \hat{\theta}$ | valeur du vecteur $\theta$ qui minimise la fonction de coÃ»t |
| $ y$ | vecteur des valeurs cible $y^{(1)}$ Ã  $y^{(m)}$ |

| Description                            | Formule |
|----------------------------------------|---------|
| **Fonction de coÃ»t MSE** pour la rÃ©gression linÃ©aire | $ MSE(X, h_\theta) = \frac{1}{m} \sum_{i=1}^{m} \left( \theta^T x^{(i)} - y^{(i)} \right)^2 $ |
| **Fonction de coÃ»t RMSE** (Racine de MSE) | $ RMSE(X, h_\theta) = \sqrt{\frac{1}{m} \sum_{i=1}^{m} \left( \theta^T x^{(i)} - y^{(i)} \right)^2} $ |
| **Ã‰quation normale** pour $\hat{\theta}$ | $ \hat{\theta} = (X^T X)^{-1} X^T y $ |


### Entrainement - MÃ©thode analytique
Calcul les valeurs des paramÃ¨tres du modÃ¨le qui donnent le meilleur rÃ©sultat sur le jeu d'entrainement (qui minimise la fonction de coÃ»t)

### Entrainement - Descente de gradient
ou Gradient Descent en anglais Optimisation itÃ©rative qui modifie graduellement les paramÃ¨tres du modÃ¨le pour minimiser la fonction de coÃ»t sur le jeu d'entrainement Converge au final vers le mÃªme jeu de paramÃ¨tres que la mÃ©thode analytique



---

##### Forme scalaire

$$\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n$$

<p>

|symbole|signification|
|:--:|:--:|
|$\hat{y}$ | prÃ©diction|
|$\theta_0$ | biais (intercept)|
|$\theta_i$ | paramÃ¨tres du modÃ¨le|
|$x_i$ | variables d'entrÃ©e|
|$n$ | nombre de variables|

</p>

---

##### Forme somme pondÃ©rÃ©e

$$\hat{y} = \sum_{i=0}^{n} \theta_i x_i$$

En rÃ©gression linÃ©aire,
on Ã©crit souvent la somme en commenÃ§ant par 0 (i=0)
avec la convention que $x_0 = 1$
pour inclure le biais $\theta_0$ dans la somme.


**biais, constante, intercept** - sont 3 termes qui dÃ©signent la mÃªme chose



|symbole|signification|
|:--:|:--:|
|$\hat{y}$ | valeur prÃ©dite|
|$\theta_i$ | i-Ã¨me paramÃ¨tre du modÃ¨le|
|$x_i$ | i-Ã¨me variable|

---

##### Forme vectorielle

On introduit le vecteur colonne des variables d'entrÃ©e, avec $ğ‘¥0=1$:

- $ğ‘‹=\begin{pmatrix} x_0 \\ x_1 \\ \vdots \\ x_n \end{pmatrix}$

Et le vecteur colonne des paramÃ¨tres du modÃ¨le :

- $\theta=\begin{pmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{pmatrix}$


Le produit scalaire entre ces deux vecteurs donne :

$$\hat{y}=ğœƒ_0â‹…ğ‘¥_0+ğœƒ_1â‹…ğ‘¥_1+â‹¯+ğœƒ_ğ‘›â‹…ğ‘¥_ğ‘›â€‹$$


---

##### Forme matricielle

$$\hat{y} =Î¸^TX$$

|symbole|signification|
|:--:|:--:|
|$X$ | vecteur des valeurs d'une observation de $x_0$ Ã  $x_n$, matrice de taille (n+1) x 1|
|$Î¸^T$ | transposÃ©s se $\theta$, matrice de taille 1 x (n+1)|



[Retour Ã  la regression](README.md#regression-lineaire)


