[Retour Ã  la regression](README.md#regression-lineaire)

#### RÃ©gression linÃ©aire
mÃ©thode simple qui modÃ©lise la relation entre une variable dÃ©pendante (en sortie) et une ou plusieurs variables indÃ©pendantes (en entrÃ©e) par une droite.

**prÃ©diction =  somme pondÃ©rÃ©e des variables d'entrÃ©e plus une constante**

Peut s'Ã©crire sous plusieurs formes:
- scalaire
- pondÃ©rÃ©e
- vectorielle
- matricielle

---

##### Forme scalaire

$$\hat{y} = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dots + \theta_n x_n$$

<p>

|symbole|signification|
|:--:|:--:|
|$\hat{y}$ | prÃ©diction|
|$\theta_0$ | biais (intercept)|
|$\theta_i$ | paramÃ¨tres du modÃ¨le|
|$x_i$ | variables d'entrÃ©e|
|$n$ | nombre de variables|

</p>

---

##### Forme somme pondÃ©rÃ©e

$$\hat{y} = \sum_{i=0}^{n} \theta_i x_i$$

En rÃ©gression linÃ©aire,
on Ã©crit souvent la somme en commenÃ§ant par 0 (i=0)
avec la convention que $x_0 = 1$
pour inclure le biais $\theta_0$ dans la somme.


**biais, constante, intercept** - sont 3 termes qui dÃ©signent la mÃªme chose



|symbole|signification|
|:--:|:--:|
|$\hat{y}$ | valeur prÃ©dite|
|$\theta_i$ | i-Ã¨me paramÃ¨tre du modÃ¨le|
|$x_i$ | i-Ã¨me variable|

---

##### Forme vectorielle

On introduit le vecteur colonne des variables d'entrÃ©e, avec $ğ‘¥0=1$:

- $ğ‘‹=\begin{pmatrix} x_0 \\ x_1 \\ \vdots \\ x_n \end{pmatrix}$

Et le vecteur colonne des paramÃ¨tres du modÃ¨le :

- $\theta=\begin{pmatrix} \theta_0 \\ \theta_1 \\ \vdots \\ \theta_n \end{pmatrix}$


Le produit scalaire entre ces deux vecteurs donne :

$$\hat{y}=ğœƒ_0â‹…ğ‘¥_0+ğœƒ_1â‹…ğ‘¥_1+â‹¯+ğœƒ_ğ‘›â‹…ğ‘¥_ğ‘›â€‹$$


---

##### Forme matricielle

$$\hat{y} =Î¸^TX$$

|symbole|signification|
|:--:|:--:|
|$X$ | vecteur des valeurs d'une observation de $x_0$ Ã  $x_n$, matrice de taille (n+1) x 1|
|$Î¸^T$ | transposÃ©s se $\theta$, matrice de taille 1 x (n+1)|



[Retour Ã  la regression](README.md#regression-lineaire)


